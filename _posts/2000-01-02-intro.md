---
title: "introduction"
bg: blue
color: white
fa-icon: quote-left
---

*Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.*

If you find this work useful, please consider citing:

<i>
Victor Campos, Brendan Jou, Xavier Giro-i-Nieto, Jordi Torres, and Shih-Fu Chang. "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", In International Conference on Learning Representations, 2018.
</i>

<pre>
@inproceedings{campos2018skip,
title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},
author={Campos, V{\'\i}ctor and Jou, Brendan and Giro-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu},
booktitle={International Conference on Learning Representations},
year={2018}
}
</pre>



Find our paper on [arXiv](https://arxiv.org/abs/1708.06834) or download the PDF directly from [here](https://github.com/imatge-upc/skiprnn-2017-telecombcn/raw/master/paper.pdf).
