---
title: "introduction"
bg: blue
color: white
fa-icon: quote-left
---

*"Insert motivational intro here" Visual saliency models aim at predicting the locations in an image that attract the attention of humans. The world contains a vast amount of noisy information and for humans to be able to interpret their daily input in a useful way the visual system has developed a filtering mechanism that allows it to focus on the most important parts of an image. By using these salient locations as soft-attention guides for other computer vision tasks a boost in performance can be naturally expected. Here we offer saliency maps from the Epic Kitchens video dataset, produced using SalGAN, an architecture that was trained on SALICON (an image dataset) using adversarial training. Despite it being originally implemented for images and able to extract exclusively static features, SalGAN outperformed all other models on the newly published video dataset DHF1K, a dataset of 1000 videos.*


Should I call for people to cite SalGAN here?

If you find this work useful, please consider citing:

<i>
Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O'Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto "SalGAN: Visual Saliency Prediction with Generative Adversarial Networks"
</i>

<pre>
@inproceedings{campos2018skip,
title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},
author={Campos, V{\'\i}ctor and Jou, Brendan and Giro-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu},
booktitle={International Conference on Learning Representations},
year={2018}
}
</pre>



Find our paper on [arXiv](https://arxiv.org/abs/1708.06834) or download the PDF directly from [here](https://github.com/imatge-upc/skiprnn-2017-telecombcn/raw/master/paper.pdf).
