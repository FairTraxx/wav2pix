---
title: "Abstract"
bg: blue
color: white
fa-icon: quote-left
---

*The first contribution of this work is to introduce a new egocentric (first person) vision dataset. The dataset consists of 7 videos spanning 34 minutes each on average, recorded using the eye gaze tracking technology that follows the movement and position of the eyes. The second goal is to extend a state of the art saliency predicting network, called SalGAN, to the task of extracting saliency maps from video datasets and evaluate its performance on the more challenging task of egocentric vision. By utilizing the aptitude of SalGAN for extracting high quality saliency maps from static features and augmenting it further with a ConvLSTM architecture on top, we wish to take advantage of the temporal information in an effort to achieve a new benchmark on video saliency.*

You mmay find detailed information about SalGAN in its original implementation paper: [arXiv](https://arxiv.org/abs/1701.01081).

<i>
Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O'Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto "SalGAN: Visual Saliency Prediction with Generative Adversarial Networks"
</i>

<pre>
@InProceedings{Pan_2017_SalGAN,
author = {Pan, Junting and Canton, Cristian and McGuinness, Kevin and O'Connor,
Noel E. and Torres, Jordi and Sayrol, Elisa and Giro-i-Nieto, Xavier and},
title = {SalGAN: Visual Saliency Prediction with Generative Adversarial Networks},
booktitle = {arXiv},
month = {January},
year = {2017}
}
</pre>


