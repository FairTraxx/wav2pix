---
title: "Abstract"
bg: blue
color: white
fa-icon: quote-left
---

*Visual saliency models aim at predicting the locations in an image that attract the attention of humans. The world contains a vast amount of noisy information and for humans to be able to interpret their daily input in a useful way the visual system has developed a filtering mechanism that allows it to focus on the most important parts of an image. By succesfully predicting and then using these salient locations as soft-attention guides for other computer vision tasks a boost in performance can be naturally expected. Here we offer saliency maps from the Epic Kitchens video dataset, produced using SalGAN, an architecture that was trained on SALICON (an image dataset) using adversarial training. Despite it being originally implemented for images and able to extract exclusively static features, SalGAN outperformed all other models on the newly published video dataset DHF1K, a dataset of 1000 videos.*

The model was originally implemented in the following work:

<i>
Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O'Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto "SalGAN: Visual Saliency Prediction with Generative Adversarial Networks"
</i>

<pre>
@InProceedings{Pan_2017_SalGAN,
author = {Pan, Junting and Canton, Cristian and McGuinness, Kevin and O'Connor,
Noel E. and Torres, Jordi and Sayrol, Elisa and Giro-i-Nieto, Xavier and},
title = {SalGAN: Visual Saliency Prediction with Generative Adversarial Networks},
booktitle = {arXiv},
month = {January},
year = {2017}
}
</pre>

Find the paper on [arXiv](https://arxiv.org/abs/1701.01081).
