---
title: "Publication"
bg: blue
color: white
fa-icon: quote-left
---

*Speech is a rich biometric signal that contains information about the identity, gender and emotional state of the speaker. In this work, we explore its potential to generate face images of a speaker by conditioning a Generative Adversarial Network (GAN) with raw speech input. We propose a deep neural network that is trained from scratch in an end-to-end fashion, generating a face directly from the raw speech waveform without any additional identity information (e.g reference image or one-hot encoding). Our model is trained in a self-supervised fashion by exploiting the audio and visual signals naturally aligned in videos. With the purpose of training from video data, we present a novel dataset collected for this work, with high-quality videos of ten youtubers with notable expressiveness in both the speech and visual signals.*


If you find this work useful, please consider citing:

<i>
Amanda Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres, Xavier Giro-i-Nieto. "Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks", ICASSP 2019.
</i>

<pre>
@InProceedings{Ventura_2019_CVPR,
author = {Ventura, Carles and Bellver, Miriam and Girbau, Andreu and Salvador, Amaia 
          and Marques, Ferran and Giro-i-Nieto, Xavier},
title = {RVOS: End-to-End Recurrent Network for Video Object Segmentation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
</pre>

Download our paper in pdf [here](https://github.com/imatge-upc/wav2pix/raw/master/wav2pix-2019-icassp.pdf) or find it on [arXiv](https://arxiv.org/abs/1903.10195).



